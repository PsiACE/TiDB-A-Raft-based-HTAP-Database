# 行式存储 TiKV

TiKV 部署系统由许多 TiKV 服务器组成。使用 Raft 在 TiKV 服务器之间进行区块复制。每台 TiKV 服务器都可以是不同区块的 Raft Leader 或 Follower 。在每台 TiKV 服务器上，数据和元数据将会保存 RocksDB ，一个可嵌入的、持久的键值存储 [5] 。每个区块都可配置最大容量，默认情况下为 96 MB 。Raft Leader 对应的 TiKV 服务器会处理相应区块的读/写请求。

当 Raft 算法响应读写请求时，基本的 Raft 过程在 Leader 与其 Follower 之间执行：

1. 区块的 Leader 从 SQL 引擎层接收请求。
2. Leader 将请求附加到其日志中。
3. Leader 向 Follower 发送新的日志条目，而 Follower 则将这些条目附加到自己的日志中。
4. Leader 等待其 Follower 的响应。如果法定数量节点成功响应，那么 Leader 就会提交请求并在本地应用它。
5. Leader 将结果发送给客户端，并继续处理传入的请求。

此过程确保数据一致性和高可用性。然而，此流程无法提供高效的性能，因为上述步骤是按顺序进行的，并且可能会产生大量的（磁盘和网络）I/O 开销。以下部分描述如何优化这个过程以实现高读/写吞吐量，即解决第 2 节中描述的第一个挑战。

## Leaders 和 Followers 之间的优化

上述过程中的第二步和第三步可以并行进行，因为二者之间没有依赖关系。因此，Leader 可以在本地附加日志的同时向追随者发送日志。即便附加日志在 Leader 上失败，只要法定人数的 Follower 能够成功附加日志，就仍然可以提交日志。在第三步中，当向 Follower 发送日志时，Leader 缓冲日志条目并批量发送给其 Follower 。 发送日志后，Leader 甚至不必等待 Follower 响应。相反，可以假设发送成功，并使用预测的日志索引发送进一步的日志。如果出现错误，Leader 将调整日志索引并重新发送复制请求。在第四步中，应用提交日志条目的 Leader 可以由不同的线程异步处理，因为在此阶段不存在一致性风险。基于上述优化，Raft 流程更新如下：

1. Leader 接收来自 SQL 引擎层的请求。
2. Leader 向 Follower 发送相应的日志，并在本地并行追加日志。
3. Leader 继续接收客户端的请求并重复步骤（2）。
4. Leader 提交日志并将其发送到要应用的另一个线程。
5. 应用日志后，Leader 将结果返回给客户端。

在优化后的过程中，来自客户端的任何请求仍然需要执行所有 Raft 步骤，但是来自多个客户端的请求是并行运行的，因此总体吞吐量会提高。

## 加速客户端的读取请求

从 TiKV Leader 读取数据具有可线性化的语义。 这意味着在时间 t 从某个区块 Leader 读取一个值时，Leader 不能在 t 之后返回读取请求的值的先前版本。这可以通过使用上面描述的 Raft 流程来实现：为每个读取请求发送一个日志条目，等待该条目提交后再返回。然而，这个过程成本很高，因为日志必须在 Raft 组中的大多数节点上完成复制，从而导致网络 I/O 的开销。为了提高性能，可以避免日志同步阶段。

Raft 保证：一旦 Leader 成功地写入数据，就可以响应任何读取请求，而无需跨服务器同步日志。然而，在 Leader 选举之后，Leader 的角色可能会在 Raft 组中的服务器之间移动。为了实现从 Leader 读取，TiKV 实现了 [29] 中描述的以下读取优化。

第一种方法称为读取索引。当 Leader 响应读取请求时，它将当前提交索引记录为本地读取索引，然后向 Follower 发送心跳消息以确认其领导地位。如果确实是 Leader ，则一旦其应用的索引大于或等于读取的索引，就可以返回值。这种方法提高了读取性能，尽管会导致一些额外的网络开销。

另一种方法是租用读取，减少了由读取索引引起心跳消息的网络开销。Leader 和 Follower 商定一个租用期限，在此期间，Follower 不发出选举请求，这样 Leader 就不会发生改变。在租用期间，Leader 可以响应任何读取请求，而无需与其追随者连接。只要每个节点上的 CPU 时钟没有太大的差异，这种方法就能很好地工作。

除了 Leader 之外，Follower 也可以响应客户端的读取请求，这被称为 Leader 读取。在 Follower 收到读取请求后，会向 Leader 询问最新的读取索引。如果本地应用索引等于或大于读取索引，则 Follower 可以将值返回给客户端；否则，必须等待要应用的日志。Follower 读取可以减轻热点区块 Leader 的压力，从而提高读取性能。然后，通过添加更多的 Follower ，可以进一步提高读取性能。

## 管理大量区块

大量区块分布在一组服务器集群上。服务器和数据大小动态变化，区块可能会聚集在其中一些服务器中，特别是 Leader 副本。从而导致这些服务器的磁盘被过度使用，而另一些则是总是空闲。此外，集群中的服务器数目也不总是固定的，可能会出现添加服务器和移除服务器的情况。

为了跨服务器平衡区块，布局驱动（Plancement Driver，PD）会对副本的数量和位置施加限制。一个关键的限制是在不同的TiKV 实例上放置至少三个区块副本，以确保高可用性。PD 通过心跳从服务器收集特定信息来完成初始化。还负责监控每台服务器的工作负载，并将热点区块迁移到不同的服务器，而不会影响应用程序。

另一方面，维护大量区块涉及发送心跳和管理元数据，这可能会导致大量的网络和存储开销。但是，如果 Raft 组没有任何工作负载，则心跳是不必要的。可以根据区块工作负载的繁忙程度调整发送心跳的频率。这可以减少遇到网络延迟或重载节点等问题的可能性。

## 动态的区块拆分与合并

某个大型区块可能变成热点区块，导致无法在合理的时间内完成读取或写入。热点区块或大型区块应该分成较小的区块，以更好地分配工作负载。另一方面，许多区块可能很小，且很少访问；然而，系统仍然需要为之维护心跳和元数据。在某些情况下，维护这些小区块会导致大量的网络和 CPU 开销。因此，有必要合并这类较小的区块。注意，为了保持区块之间的顺序，只合并键空间中的相邻区域。基于监测到的工作负载，PD 动态地向 TiKV 发送拆分和合并指令。拆分操作将区块划分为几个新的、更小的区块，每个区块覆盖原区块中连续的键范围。覆盖最右范围的区块会重用原区块的 Raft 组。其他区块使用新的 Raft 组。拆分流程类似于 Raft 流程中的正常更新请求：

1. PD 向区块的 Leader 发出拆分指令。
2. 在接收到拆分指令后，Leader 将指令转换为日志，并将日志复制到其所有 Follower 节点。日志只包含拆分指令，而不会修改实际数据。
3. 一旦超过法定数目的节点复制了日志，Leader 就会提交拆分指令，该指令将应用于 Raft 组中的所有节点。应用过程包括更新原区块的范围和时间戳元数据，并创建新区块以覆盖剩余范围。注意，该指令是以原子方式应用并同步到磁盘上的。
4. 对于拆分区块的每个副本，创建 Raft 状态机并开始工作，形成新的 Raft 组。 原大型区块的 Leader 向 PD 报告拆分结果。拆分流程完成。

注意，当大多数节点提交拆分日志时，拆分过程将会成功实施。和提交其他 Raft 日志类似，而不会要求所有节点完成对区块的拆分。拆分后，如果对网络进行分区，则具有最近时间戳的节点组会胜出。区块拆分的开销很低，因为只需要更改元数据。在拆分指令完成后，由于 PD 的常规负载平衡，新拆分的区域可能会跨服务器移动。

合并两个相邻区块和拆分一个区块相反。PD 移动两个区块的副本，并置到单独的服务器上。然后，通过两阶段操作，在每台服务器上本地合并两个区块的并置副本：即停止其一区块的服务并将其与另一区块合并。这种方法不同于拆分区块，因为无法使用两个 Raft 组之间的日志复制过程来准许合并操作。
